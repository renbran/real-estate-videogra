const fs = require('fs').promises;
const path = require('path');
const { pool } = require('../config/database');
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.json(),
  transports: [
    new winston.transports.File({ filename: 'logs/file-cleanup.log' })
  ]
});

// Clean up old files (older than specified days)
async function cleanupOldFiles(daysOld = 90) {
  try {
    logger.info(`Starting file cleanup for files older than ${daysOld} days`);
    
    const cutoffDate = new Date();
    cutoffDate.setDate(cutoffDate.getDate() - daysOld);
    
    // Get old files from database
    const oldFilesResult = await pool.query(`
      SELECT id, filename, file_path, original_filename, file_size, uploaded_at
      FROM file_storage 
      WHERE uploaded_at < $1
        AND last_accessed < $2 OR last_accessed IS NULL
    `, [cutoffDate, cutoffDate]);
    
    if (oldFilesResult.rows.length === 0) {\n      logger.info('No old files found for cleanup');\n      return { success: true, filesDeleted: 0, spaceFreed: 0 };\n    }\n    \n    let filesDeleted = 0;\n    let spaceFreed = 0;\n    let errors = [];\n    \n    for (const file of oldFilesResult.rows) {\n      try {\n        // Check if file still exists on disk\n        try {\n          await fs.access(file.file_path);\n        } catch {\n          // File doesn't exist on disk, just remove from database\n          await pool.query('DELETE FROM file_storage WHERE id = $1', [file.id]);\n          logger.info(`Removed database record for missing file: ${file.original_filename}`);\n          continue;\n        }\n        \n        // Delete file from disk\n        await fs.unlink(file.file_path);\n        \n        // Remove from database\n        await pool.query('DELETE FROM file_storage WHERE id = $1', [file.id]);\n        \n        filesDeleted++;\n        spaceFreed += file.file_size;\n        \n        logger.info(`Deleted old file: ${file.original_filename}`, {\n          fileId: file.id,\n          size: file.file_size,\n          uploadedAt: file.uploaded_at\n        });\n        \n      } catch (error) {\n        errors.push({\n          fileId: file.id,\n          filename: file.original_filename,\n          error: error.message\n        });\n        \n        logger.error(`Failed to delete file ${file.original_filename}:`, error);\n      }\n    }\n    \n    // Clean up empty directories\n    await cleanupEmptyDirectories();\n    \n    // Clean up old log files\n    await cleanupOldLogFiles();\n    \n    // Clean up old backup files\n    await cleanupOldBackups();\n    \n    const result = {\n      success: true,\n      filesDeleted,\n      spaceFreed,\n      spaceFreedMB: (spaceFreed / (1024 * 1024)).toFixed(2),\n      errors\n    };\n    \n    logger.info('File cleanup completed', result);\n    \n    // Log cleanup summary to database\n    await pool.query(`\n      INSERT INTO backup_logs (backup_type, backup_size, backup_location, status)\n      VALUES ($1, $2, $3, $4)\n    `, [\n      'file_cleanup',\n      spaceFreed,\n      `Cleaned ${filesDeleted} files`,\n      'completed'\n    ]);\n    \n    return result;\n    \n  } catch (error) {\n    logger.error('File cleanup process failed:', error);\n    throw error;\n  }\n}\n\n// Clean up empty directories in the uploads folder\nasync function cleanupEmptyDirectories() {\n  try {\n    const uploadsDir = path.join(__dirname, '..', 'uploads');\n    \n    async function removeEmptyDirs(dir) {\n      try {\n        const items = await fs.readdir(dir);\n        \n        // Recursively check subdirectories\n        for (const item of items) {\n          const itemPath = path.join(dir, item);\n          const stat = await fs.stat(itemPath);\n          \n          if (stat.isDirectory()) {\n            await removeEmptyDirs(itemPath);\n            \n            // Check if directory is now empty\n            const remainingItems = await fs.readdir(itemPath);\n            if (remainingItems.length === 0 && itemPath !== uploadsDir) {\n              await fs.rmdir(itemPath);\n              logger.info(`Removed empty directory: ${itemPath}`);\n            }\n          }\n        }\n      } catch (error) {\n        // Directory might not exist or be inaccessible\n        logger.warn(`Could not process directory ${dir}:`, error.message);\n      }\n    }\n    \n    await removeEmptyDirs(uploadsDir);\n    \n  } catch (error) {\n    logger.error('Failed to cleanup empty directories:', error);\n  }\n}\n\n// Clean up old log files\nasync function cleanupOldLogFiles(daysOld = 30) {\n  try {\n    const logsDir = path.join(__dirname, '..', 'logs');\n    const cutoffDate = new Date();\n    cutoffDate.setDate(cutoffDate.getDate() - daysOld);\n    \n    try {\n      const logFiles = await fs.readdir(logsDir);\n      \n      for (const file of logFiles) {\n        const filePath = path.join(logsDir, file);\n        const stat = await fs.stat(filePath);\n        \n        if (stat.isFile() && stat.mtime < cutoffDate) {\n          await fs.unlink(filePath);\n          logger.info(`Deleted old log file: ${file}`);\n        }\n      }\n    } catch (error) {\n      // Logs directory might not exist\n      logger.warn('Could not cleanup log files:', error.message);\n    }\n    \n  } catch (error) {\n    logger.error('Failed to cleanup old log files:', error);\n  }\n}\n\n// Clean up old backup files\nasync function cleanupOldBackups(daysOld = 60) {\n  try {\n    const backupsDir = path.join(__dirname, '..', 'backups');\n    const cutoffDate = new Date();\n    cutoffDate.setDate(cutoffDate.getDate() - daysOld);\n    \n    try {\n      const backupFiles = await fs.readdir(backupsDir);\n      \n      // Keep at least 5 most recent backups\n      const backupStats = [];\n      \n      for (const file of backupFiles) {\n        const filePath = path.join(backupsDir, file);\n        const stat = await fs.stat(filePath);\n        \n        if (stat.isFile() && (file.endsWith('.sql') || file.endsWith('.gz') || file.endsWith('.zip'))) {\n          backupStats.push({\n            file,\n            path: filePath,\n            mtime: stat.mtime,\n            size: stat.size\n          });\n        }\n      }\n      \n      // Sort by modification time (newest first)\n      backupStats.sort((a, b) => b.mtime - a.mtime);\n      \n      // Keep first 5, delete old ones\n      const toDelete = backupStats.slice(5).filter(backup => backup.mtime < cutoffDate);\n      \n      for (const backup of toDelete) {\n        await fs.unlink(backup.path);\n        logger.info(`Deleted old backup file: ${backup.file}`);\n      }\n      \n    } catch (error) {\n      // Backups directory might not exist\n      logger.warn('Could not cleanup backup files:', error.message);\n    }\n    \n  } catch (error) {\n    logger.error('Failed to cleanup old backup files:', error);\n  }\n}\n\n// Clean up temporary files\nasync function cleanupTempFiles() {\n  try {\n    const tempDir = path.join(__dirname, '..', 'temp');\n    const cutoffTime = new Date();\n    cutoffTime.setHours(cutoffTime.getHours() - 24); // Delete files older than 24 hours\n    \n    try {\n      const tempFiles = await fs.readdir(tempDir);\n      \n      for (const file of tempFiles) {\n        const filePath = path.join(tempDir, file);\n        const stat = await fs.stat(filePath);\n        \n        if (stat.isFile() && stat.mtime < cutoffTime) {\n          await fs.unlink(filePath);\n          logger.info(`Deleted temp file: ${file}`);\n        }\n      }\n    } catch (error) {\n      // Temp directory might not exist\n      logger.warn('Could not cleanup temp files:', error.message);\n    }\n    \n  } catch (error) {\n    logger.error('Failed to cleanup temp files:', error);\n  }\n}\n\n// Get storage usage statistics\nasync function getStorageUsage() {\n  try {\n    // Get database file statistics\n    const dbStatsResult = await pool.query(`\n      SELECT \n        COUNT(*) as total_files,\n        SUM(file_size) as total_size,\n        AVG(file_size) as avg_file_size,\n        MIN(uploaded_at) as oldest_file,\n        MAX(uploaded_at) as newest_file,\n        COUNT(CASE WHEN last_accessed IS NULL THEN 1 END) as never_accessed,\n        COUNT(CASE WHEN last_accessed < NOW() - INTERVAL '30 days' THEN 1 END) as not_accessed_30_days\n      FROM file_storage\n    `);\n    \n    const dbStats = dbStatsResult.rows[0];\n    \n    // Get physical directory sizes\n    const uploadsDir = path.join(__dirname, '..', 'uploads');\n    const logsDir = path.join(__dirname, '..', 'logs');\n    const backupsDir = path.join(__dirname, '..', 'backups');\n    \n    const uploadsDirSize = await getDirectorySize(uploadsDir);\n    const logsDirSize = await getDirectorySize(logsDir);\n    const backupsDirSize = await getDirectorySize(backupsDir);\n    \n    return {\n      database: {\n        totalFiles: parseInt(dbStats.total_files),\n        totalSizeBytes: parseInt(dbStats.total_size || 0),\n        totalSizeMB: ((dbStats.total_size || 0) / (1024 * 1024)).toFixed(2),\n        avgFileSizeBytes: parseFloat(dbStats.avg_file_size || 0),\n        oldestFile: dbStats.oldest_file,\n        newestFile: dbStats.newest_file,\n        neverAccessed: parseInt(dbStats.never_accessed),\n        notAccessed30Days: parseInt(dbStats.not_accessed_30_days)\n      },\n      directories: {\n        uploads: {\n          sizeBytes: uploadsDirSize,\n          sizeMB: (uploadsDirSize / (1024 * 1024)).toFixed(2)\n        },\n        logs: {\n          sizeBytes: logsDirSize,\n          sizeMB: (logsDirSize / (1024 * 1024)).toFixed(2)\n        },\n        backups: {\n          sizeBytes: backupsDirSize,\n          sizeMB: (backupsDirSize / (1024 * 1024)).toFixed(2)\n        }\n      },\n      total: {\n        sizeBytes: uploadsDirSize + logsDirSize + backupsDirSize,\n        sizeMB: ((uploadsDirSize + logsDirSize + backupsDirSize) / (1024 * 1024)).toFixed(2),\n        sizeGB: ((uploadsDirSize + logsDirSize + backupsDirSize) / (1024 * 1024 * 1024)).toFixed(2)\n      }\n    };\n    \n  } catch (error) {\n    logger.error('Failed to get storage usage:', error);\n    throw error;\n  }\n}\n\n// Calculate directory size recursively\nasync function getDirectorySize(dirPath) {\n  try {\n    let totalSize = 0;\n    \n    const items = await fs.readdir(dirPath);\n    \n    for (const item of items) {\n      const itemPath = path.join(dirPath, item);\n      const stat = await fs.stat(itemPath);\n      \n      if (stat.isDirectory()) {\n        totalSize += await getDirectorySize(itemPath);\n      } else {\n        totalSize += stat.size;\n      }\n    }\n    \n    return totalSize;\n    \n  } catch (error) {\n    // Directory might not exist\n    return 0;\n  }\n}\n\n// Cleanup orphaned files (files on disk not in database)\nasync function cleanupOrphanedFiles() {\n  try {\n    logger.info('Starting orphaned files cleanup');\n    \n    const uploadsDir = path.join(__dirname, '..', 'uploads');\n    const orphanedFiles = [];\n    \n    // Get all file paths from database\n    const dbFilesResult = await pool.query('SELECT file_path FROM file_storage');\n    const dbFilePaths = new Set(dbFilesResult.rows.map(row => path.resolve(row.file_path)));\n    \n    // Recursively scan uploads directory\n    async function scanDirectory(dir) {\n      try {\n        const items = await fs.readdir(dir);\n        \n        for (const item of items) {\n          const itemPath = path.join(dir, item);\n          const stat = await fs.stat(itemPath);\n          \n          if (stat.isDirectory()) {\n            await scanDirectory(itemPath);\n          } else {\n            const resolvedPath = path.resolve(itemPath);\n            \n            // Skip system files and thumbnails\n            if (!item.startsWith('.') && !item.includes('_thumb') && !dbFilePaths.has(resolvedPath)) {\n              orphanedFiles.push({\n                path: itemPath,\n                size: stat.size,\n                mtime: stat.mtime\n              });\n            }\n          }\n        }\n      } catch (error) {\n        logger.warn(`Could not scan directory ${dir}:`, error.message);\n      }\n    }\n    \n    await scanDirectory(uploadsDir);\n    \n    // Delete orphaned files older than 7 days\n    const cutoffDate = new Date();\n    cutoffDate.setDate(cutoffDate.getDate() - 7);\n    \n    let deletedCount = 0;\n    let spaceFreed = 0;\n    \n    for (const file of orphanedFiles) {\n      if (file.mtime < cutoffDate) {\n        try {\n          await fs.unlink(file.path);\n          deletedCount++;\n          spaceFreed += file.size;\n          logger.info(`Deleted orphaned file: ${file.path}`);\n        } catch (error) {\n          logger.error(`Failed to delete orphaned file ${file.path}:`, error);\n        }\n      }\n    }\n    \n    logger.info(`Orphaned files cleanup completed: ${deletedCount} files deleted, ${(spaceFreed / (1024 * 1024)).toFixed(2)}MB freed`);\n    \n    return {\n      success: true,\n      orphanedFound: orphanedFiles.length,\n      orphanedDeleted: deletedCount,\n      spaceFreed: spaceFreed\n    };\n    \n  } catch (error) {\n    logger.error('Orphaned files cleanup failed:', error);\n    throw error;\n  }\n}\n\nmodule.exports = {\n  cleanupOldFiles,\n  cleanupTempFiles,\n  cleanupOrphanedFiles,\n  getStorageUsage,\n  cleanupEmptyDirectories,\n  cleanupOldLogFiles,\n  cleanupOldBackups\n};"